{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e10397a",
   "metadata": {},
   "source": [
    "# Lab 2 - Cross validation \n",
    "\n",
    "The following functions are the implementation  of the KNN algorithm for classification problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efd1357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages used\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from scipy.interpolate import griddata\n",
    "import os\n",
    "import pickle\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21793caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearBinaryClass(n, low_D, high_D, m, q):\n",
    "    X = (np.random.rand(n, 2) * (high_D - low_D)) + low_D\n",
    "    Y = np.sign((X[:,1] - (m * X[:,0]) + q))\n",
    "    Y[Y==0] = 1\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab66b6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidDistance(P1,P2):\n",
    "    return np.linalg.norm(P1-P2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a29b3fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allDistances(X1, X2):\n",
    "    D = np.zeros((X1.shape[0], X2.shape[0]))\n",
    "    for idx1 in range(len(X1)):\n",
    "        for idx2 in range(len(X2)):\n",
    "            D[idx1,idx2] = euclidDistance(X1[idx1,:],X2[idx2,:])\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "732662c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flipLabels(Y, P):\n",
    "    if P < 1 or P > 100:\n",
    "        raise Exception('P should be between 1 and 100')\n",
    "        \n",
    "    indices_to_flip = np.random.choice(range(len(Y)), int(len(Y) * (P / 100)), replace=False)\n",
    "    Y_noisy = Y.copy()\n",
    "    Y_noisy[indices_to_flip] *= -1\n",
    "    \n",
    "    return Y_noisy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd91f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNNClassify(Xtr, Ytr, k, Xte):\n",
    "\n",
    "    n_train = Xtr.shape[0] # number of the training inputs\n",
    "    n_test = Xte.shape[0] # number of the test inputs\n",
    "\n",
    "    if any(np.abs(Ytr) != 1):\n",
    "        raise Exception(\"The values of Ytr should be +1 or -1.\")\n",
    "\n",
    "    if k > n_train:\n",
    "        print(\"k is greater than the number of points, setting k=n_train\")\n",
    "        k = n_train\n",
    "\n",
    "    Ypred = np.zeros(n_test)\n",
    "\n",
    "    # Compute all the distances from test input and training input\n",
    "    dist = allDistances(Xte, Xtr)\n",
    "    \n",
    "    # For each test point, the predicted class will be \n",
    "    # the sign of the average label of the k nearest points\n",
    "    for idx in range(n_test):\n",
    "        # All the distances for the current test point\n",
    "        idx_dist = dist[idx]\n",
    "        # Sorting the indices of the distances\n",
    "        order_idx_dist = np.argsort(idx_dist)\n",
    "        \n",
    "        # Getting the k lowest distence's indices\n",
    "        k_idx_dist = order_idx_dist[:k]\n",
    "        # Computing the mean of the output values of the corresponding training points and taking the sign of it.\n",
    "        Ypred[idx] = np.sign(np.mean(Ytr[k_idx_dist]))\n",
    "    return Ypred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff7531e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separatingFkNN(Xtr, Ytr, k, Xte):\n",
    "    Ypred = kNNClassify(Xtr=Xtr, Ytr=Ytr, k=k, Xte=Xte)\n",
    "\n",
    "    x = Xtr[:, 0]\n",
    "    y = Xtr[:, 1]\n",
    "    xi = np.linspace(x.min(), x.max(), 500)\n",
    "    yi = np.linspace(y.min(), y.max(), 500)\n",
    "    zi = griddata((x, y), Ypred, (xi[None, :], yi[:, None]), method='linear')\n",
    "\n",
    "    plt.subplots()\n",
    "    CS = plt.contour(xi, yi, zi, 15, linewidths=2, colors='k', levels=[0])\n",
    "    # plot data points.\n",
    "    #plt.scatter(x, y, c=Ytr, marker='o', s=50, zorder=10, alpha=0.8)\n",
    "    plt.scatter(x[Ytr==1], y[Ytr==1], c=\"#C59434\", marker='o', s=50, zorder=10, alpha=0.8)\n",
    "    plt.scatter(x[Ytr==-1], y[Ytr==-1], c=\"#092C48\", marker='o', s=50, zorder=10, alpha=0.8)\n",
    "    plt.xlim(x.min(), x.max())\n",
    "    plt.ylim(x.min(), x.max())\n",
    "    #msg = 'Separating function, k='+str(k);\n",
    "    #plt.title(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b444c6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(Xtr, Ytr):\n",
    "    x = Xtr[:, 0]\n",
    "    y = Xtr[:, 1]\n",
    "    plt.scatter(x[Ytr==1], y[Ytr==1], c=\"#C59434\", marker='o', s=50, zorder=10, alpha=0.8)\n",
    "    plt.scatter(x[Ytr==-1], y[Ytr==-1], c=\"#092C48\", marker='o', s=50, zorder=10, alpha=0.8)\n",
    "    plt.xlim(x.min(), x.max())\n",
    "    plt.ylim(x.min(), x.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "485a06ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcError(Ypred, Ytrue):\n",
    "    return (np.count_nonzero(Ypred != Ytrue)) / len(Ytrue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daf0180",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "In this Lab, we focus on Cross-validation for model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa644a6",
   "metadata": {},
   "source": [
    "### Preliminaries: Data-loading\n",
    "\n",
    "There are 4 datasets included with this lab, with the following properties:\n",
    "\n",
    "  - (Training1, Test1): n=70, no noise, suggested K values are in the range 1 ... 13\n",
    "  - (Training2, Test2): n=40, no noise, suggested K values are in the range 1 ... 23\n",
    "  - (Training3, Test3): n=200, 20% flipped labels, suggested K values are in the range 1 ... 31\n",
    "  - (Training4, Test4): n=200, 5% flipped labels, suggested K values are in the range 1 ... 19\n",
    "\n",
    "Note that each dataset includes both a training and a test set. You **cannot use the test set to choose k**: you should assume you don't have access to the test set when doing model selection!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34d7b77",
   "metadata": {},
   "source": [
    "To load a dataset use function\n",
    "`Xtr, Ytr, Xte, Yte = load_dataset(path_tr, path_te)`\n",
    "\n",
    "Where:\n",
    "- **_path\\_tr_**: is the path to the file which contains the training set\n",
    "- **_path\\_te_**: is the path to the file which contains the test set\n",
    "\n",
    "and it returns:\n",
    "- **_Xtr_**: Inputs of training set\n",
    "- **_Ytr_**: Outputs of training set\n",
    "- **_Xte_**: Inputs of test set\n",
    "- **_Yte_**: Outputs of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ce1fe93f1cfd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "### If you use colab uncomment these line and upload Material.zip\n",
    "\n",
    "#from google.colab import files\n",
    "\n",
    "#files.upload()\n",
    "#!unzip Material.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e714e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path_tr, path_te):\n",
    "    with open(path_tr,\"rb\") as f:\n",
    "        [Xtr, Ytr] = pickle.load(f)\n",
    "    with open(path_te,\"rb\") as f:\n",
    "        [Xte, Yte] = pickle.load(f)\n",
    "    return Xtr, Ytr, Xte, Yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90828340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "training_set_path = \"./Material/Classification/Training1.dat\"\n",
    "test_set_path = \"./Material/Classification/Test1.dat\"\n",
    "Xtr, Ytr, Xte, Yte = load_dataset(training_set_path, test_set_path)\n",
    "separatingFkNN(Xtr, Ytr, 1, Xtr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba5bf88",
   "metadata": {},
   "source": [
    "## Using the training set to choose k\n",
    "\n",
    "Having noted that we cannot directly use the test set to find the best k, we can try to select it with the training set. \n",
    "Follow these steps, and fill in the blanks in the following cells. **Comment on your findings in the notebook**.\n",
    "\n",
    " 1. Choose a list of values for k you want to try. Check the list of suggested values for any given dataset.\n",
    " 2. For each value of k do:\n",
    "     1. Train a k-NN classifier with the current value of k\n",
    "     2. Evaluate the error of the trained model, **on the training set**\n",
    " 3. The _best_ k is the one with the lowest training error!\n",
    " 4. Train a k-NN model with the chosen k, and evaluate the error **on the test set**. Is the test error similar to the training error? Does the chosen k seem like a good one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6668878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Use dataset 1 for this task.\n",
    "Xtr, Ytr, Xte, Yte = load_dataset(\n",
    "    \"./Material/Classification/Training1.dat\", \"./Material/Classification/Test1.dat\")\n",
    "k_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
    "\n",
    "k_tr_errors = []\n",
    "for current_k in k_list:\n",
    "    Ypred = kNNClassify(Xtr, Ytr, current_k, Xtr)\n",
    "    current_tr_err = np.mean(Ypred!= Ytr) # , calculate the error on the training set.\n",
    "    k_tr_errors.append(current_tr_err)\n",
    "    \n",
    "# Assuming k_list and k_tr_errors are defined\n",
    "best_k = k_list[k_tr_errors.index(min(k_tr_errors))]\n",
    "\n",
    "print(f\"The best k chosen with the training-set is: {best_k}\\n\"\n",
    "      f\"with training error {np.min(k_tr_errors)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "056d4159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Evaluate the model with the best k on the test set now\n",
    "\n",
    "Ypred = kNNClassify(Xtr, Ytr, best_k, Xte)\n",
    "te_err = calcError(Ypred, Yte)\n",
    "    \n",
    "print(f\"The test error with the best k ({best_k}) is: {te_err*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a96f7460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Plot how the training error changes with k\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(k_list, k_tr_errors)\n",
    "ax.set_xlabel(\"k\")\n",
    "ax.set_ylabel(\"training error\")\n",
    "ax.set_title(\"Choosing k from the training set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300842fd",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d31aa02",
   "metadata": {},
   "source": [
    "## K-Fold cross validation\n",
    "\n",
    "The training error does not provide good estimates of the test error. So using the training error for model selection means you will select a k which is not good for the test set.\n",
    "\n",
    "A simple way, which is much better than the training error, to estimate the test error is to use a **k-fold cross validation**. In K-Fold cross validation, the data are split into K parts of approximately equal size, and K different models are trained each time leaving out one of the parts of data. Then the estimate for the test error is the mean of the error of the K different models.\n",
    "\n",
    "In this task you will implement K-Fold CV, and run the same analysis as in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14052b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFoldCVkNN(Xtr, Ytr, num_folds, hyperparam_list):\n",
    "    \"\"\"Run K-Fold CV for the kNN model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "     - Xtr : np.array\n",
    "         the full training set data\n",
    "     - Ytr : np.array\n",
    "         the full training set labels\n",
    "     - num_folds : int\n",
    "         the number of folds\n",
    "     - hyperparam_list : List[int]\n",
    "         the values of k (for k-NN) to try.\n",
    "         \n",
    "    Returns:\n",
    "    --------\n",
    "     - best_k : int\n",
    "         The value of k (in k_list) which obtains the best average validation error\n",
    "     - best_k_idx : int\n",
    "         The index of the best_k element in k_list\n",
    "     - tr_err_mean : np.array\n",
    "         A 1D array of the same length as k_list, with the average training error for each tested k.\n",
    "     - tr_err_std : np.array\n",
    "         A 1D array of the same length as k_list, with the standard deviation \n",
    "         of the training error for each tested k.\n",
    "     - val_err_mean : np.array\n",
    "         A 1D array of the same length as k_list, with the average validation error for each tested k.\n",
    "     - val_err_std : np.array\n",
    "         A 1D array of the same length as k_list, with the standard deviation\n",
    "         of the validation error for each tested k.\n",
    "    \"\"\"\n",
    "    rnd_state = np.random.RandomState()\n",
    "    # Ensures that k_list is a numpy array\n",
    "    hyperparam_list = np.array(hyperparam_list)\n",
    "    num_k = len(hyperparam_list)\n",
    "\n",
    "    n_tot = Xtr.shape[0]\n",
    "\n",
    "    # We want to compute 1 error for each `k` and each fold\n",
    "    tr_errors = np.zeros((num_k, num_folds))\n",
    "    val_errors = np.zeros((num_k, num_folds))\n",
    "\n",
    "    # `split_idx`: a list of arrays, each containing the validation indices for 1 fold\n",
    "    rand_idx = rnd_state.choice(n_tot, size=n_tot, replace=False)\n",
    "    split_idx = np.array_split(rand_idx, num_folds)\n",
    "    \n",
    "    for fold_idx in range(num_folds):\n",
    "        # Set the indices in boolean mask for all validation samples to `True`\n",
    "        val_mask = np.zeros(n_tot, dtype=bool)\n",
    "        val_mask[split_idx[fold_idx]] = True\n",
    "        # Split training set in training part and validation part\n",
    "        # Hint: you can use boolean mask as index vector to split Xtr and Ytr\n",
    "        x_train = Xtr[val_mask==False]\n",
    "        y_train = Ytr[val_mask==False]\n",
    "        x_val = Xtr[val_mask==True]\n",
    "        y_val = Ytr[val_mask==True]\n",
    "        \n",
    "        for k_idx, current_k in enumerate(hyperparam_list):\n",
    "            # Compute the training error of the kNN classifier for the given value of k\n",
    "            tr_errors[k_idx, fold_idx] = calcError(kNNClassify(x_train, y_train, current_k, x_train), y_train)\n",
    "            val_errors[k_idx, fold_idx] = calcError(kNNClassify(x_train, y_train, current_k, x_val), y_val)\n",
    "            \n",
    "    # Calculate error statistics along the repetitions:\n",
    "    # 1) mean training error, training error standard deviation\n",
    "    tr_err_mean = np.mean(tr_errors, axis=1)\n",
    "    tr_err_std = np.std(tr_errors, axis=1)\n",
    "    # 2) mean validation error, validation error standard deviation\n",
    "    val_err_mean = np.mean(val_errors, axis=1)\n",
    "    val_err_std = np.std(val_errors, axis=1)\n",
    "    # 3) best k (k which minimize mean validation error) and index of best k in hyperparam_list\n",
    "    best_k = hyperparam_list[np.argmin(val_err_mean)]\n",
    "    best_k_idx = np.argmin(val_err_mean)\n",
    "    \n",
    "    return best_k, best_k_idx, tr_err_mean, tr_err_std, val_err_mean, val_err_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d892e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "Xtr, Ytr, Xte, Yte = load_dataset(\n",
    "    \"./Material/Classification/Training1.dat\", \"./Material/Classification/Test1.dat\")\n",
    "\n",
    "num_folds = 5\n",
    "k_list = range(1, 23)\n",
    "best_k, best_k_idx, tr_err_mean, tr_err_std, val_err_mean, val_err_std = KFoldCVkNN(\n",
    "    Xtr, Ytr, num_folds, k_list)\n",
    "\n",
    "print(f\"The best k chosen with {num_folds} folds is {best_k} with error \"\n",
    "      f\"{np.min(val_err_mean) * 100:.2f} +- {val_err_std[np.argmin(val_err_mean)] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2795038fc2eb05f1",
   "metadata": {},
   "source": [
    "### Plot the training and validation error with the K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29733379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(k_list, val_err_mean, label=\"Validation\")\n",
    "ax.fill_between(k_list, val_err_mean - val_err_std, val_err_mean + val_err_std, alpha=0.2)\n",
    "ax.plot(k_list, tr_err_mean, label=\"Training\")\n",
    "ax.fill_between(k_list, tr_err_mean - tr_err_std, tr_err_mean + tr_err_std, alpha=0.2)\n",
    "\n",
    "ax.set_title(f\"{num_folds}-Fold CV\")\n",
    "ax.set_ylabel(\"Error\")\n",
    "ax.set_xlabel(\"k\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a812833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Evaluate the model with the best k on the test set now\n",
    "Ypred = kNNClassify(Xtr, Ytr, best_k, Xte)\n",
    "te_err = calcError(Ypred, Yte)\n",
    "    \n",
    "print(f\"The test error with the best k ({best_k}) is: {te_err*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27091a63",
   "metadata": {},
   "source": [
    "## Cross validation and Noise in k-NN\n",
    "\n",
    "In this final task we consider the effect of noise on the best k (chosen with k-fold CV).\n",
    "\n",
    "We will use dataset 3 which has high noise (20%), and dataset 4 which has low noise (5%).\n",
    "\n",
    "You will have to do the following:\n",
    " 1. Use k-fold CV to find the best k for datasets 3 and 4.\n",
    " 2. Compare the obtained values of k, with the actual best k on the test error of the two datasets. This should be a sanity check: if the errors on k-fold CV and on the test set are very different, something may be wrong with your code.\n",
    " 3. Comment on how noise affects the best k: does a more noisy dataset need a higher or a lower k, and why? It may be useful to plot the separating function (using the `separatingFkNN` function) of k-NN with the best k for the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06bdcd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "k_list = range(1,19,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54b8123",
   "metadata": {},
   "source": [
    "**Dataset 3 (high noise)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7c9ee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use k-fold CV to find best k for dataset 3 (high noise)\n",
    "Xtr, Ytr, Xte, Yte = load_dataset(\"./Material/Classification/Training3.dat\", \"./Material/Classification/Test3.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72475331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# TODO: Find the best k on the test set of dataset 3\n",
    "best_k, best_k_idx, tr_err_mean, tr_err_std, val_err_mean, val_err_std = KFoldCVkNN(\n",
    "    Xtr, Ytr, num_folds, k_list)\n",
    "print(f\"The best k chosen with {num_folds} folds is {best_k} with error \"\n",
    "      f\"{np.min(val_err_mean) * 100:.2f} +- {val_err_std[np.argmin(val_err_mean)] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "217161d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Sanity check - Computing the test error\n",
    "test_error = calcError(kNNClassify(Xtr, Ytr, best_k, Xte), Yte)\n",
    "print(f'The test error with k={best_k} is {test_error * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63182b4258a8042c",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# plot the errors with K\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(k_list, val_err_mean, label=\"Validation\")\n",
    "ax.fill_between(k_list, val_err_mean - val_err_std, val_err_mean + val_err_std, alpha=0.2)\n",
    "ax.plot(k_list, tr_err_mean, label=\"Training\")\n",
    "ax.fill_between(k_list, tr_err_mean - tr_err_std, tr_err_mean + tr_err_std, alpha=0.2)\n",
    "\n",
    "ax.set_title(f\"{num_folds}-Fold CV\")\n",
    "ax.set_ylabel(\"Error\")\n",
    "ax.set_xlabel(\"k\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fca8a281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Plot separatingFkNN on the TRAINING SET\n",
    "separatingFkNN(Xtr, Ytr, best_k, Xtr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794bc3e2300f256e",
   "metadata": {},
   "source": [
    "Now select the best K using only Xtr but without K-fold, as already did at the beginning of the notebook. What do you observe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "713e292c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Use dataset 3 for this task.\n",
    "Xtr, Ytr, Xte, Yte = load_dataset(\"./Material/Classification/Training3.dat\", \"./Material/Classification/Test3.dat\")\n",
    "k_list = range(1, 19, 2)\n",
    "\n",
    "k_tr_errors = []\n",
    "for current_k in k_list:\n",
    "    Ypred = kNNClassify(Xtr, Ytr, current_k, Xtr)\n",
    "    current_tr_err = calcError(Ypred, Ytr)  # Calculate the error on the training set.\n",
    "    k_tr_errors.append(current_tr_err)\n",
    "\n",
    "# Assuming k_list and k_tr_errors are defined\n",
    "best_k = k_list[k_tr_errors.index(min(k_tr_errors))]\n",
    "\n",
    "print(f\"The best k chosen with the training-set is: {best_k}\\n\"\n",
    "      f\"with training error {np.min(k_tr_errors) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4351f70d",
   "metadata": {},
   "source": [
    "**Dataset 4 (low noise)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2fe1c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use k-fold CV to find best k for dataset 4 (low noise)\n",
    "Xtr, Ytr, Xte, Yte = load_dataset(\n",
    "    \"./Material/Classification/Training4.dat\", \"./Material/Classification/Test4.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6027afbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# TODO: Find the best k on the test set of dataset 4\n",
    "best_k, best_k_idx, tr_err_mean, tr_err_std, val_err_mean, val_err_std = KFoldCVkNN(\n",
    "    Xtr, Ytr, num_folds, k_list)    \n",
    "print(f\"The best k chosen with {num_folds} folds is {best_k} with error \"\n",
    "      f\"{np.min(val_err_mean) * 100:.2f} +- {val_err_std[np.argmin(val_err_mean)] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4edf05f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Sanity check - Computing the test error\n",
    "test_error = calcError(kNNClassify(Xtr, Ytr, best_k, Xte), Yte)\n",
    "print(f'The test error with k={best_k} is {test_error * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d5e76527b7ab91",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77433299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Plot separatingFkNN\n",
    "separatingFkNN(Xtr, Ytr, best_k, Xtr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393f2e41f2b521c4",
   "metadata": {},
   "source": [
    "Now select the best K using only Xtr but without K-fold, as already did at the beginning of the notebook. What do you observe?\n",
    "How does noise affect the best k?\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da515eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Use dataset 4 for this task.\n",
    "Xtr, Ytr, Xte, Yte = load_dataset(\"./Material/Classification/Training4.dat\", \"./Material/Classification/Test4.dat\")\n",
    "k_list = range(1, 19, 2)\n",
    "\n",
    "k_tr_errors = []\n",
    "for current_k in k_list:\n",
    "    Ypred = kNNClassify(Xtr, Ytr, current_k, Xtr)\n",
    "    current_tr_err = calcError(Ypred, Ytr)  # Calculate the error on the training set.\n",
    "    k_tr_errors.append(current_tr_err)\n",
    "\n",
    "# Assuming k_list and k_tr_errors are defined\n",
    "best_k = k_list[k_tr_errors.index(min(k_tr_errors))]\n",
    "\n",
    "print(f\"The best k chosen with the training-set is: {best_k}\\n\"\n",
    "      f\"with training error {np.min(k_tr_errors) * 100:.2f}%\")\n",
    "\n",
    "separatingFkNN(Xtr, Ytr, best_k, Xtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13fc6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "111725037dec6740",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fa8c7df",
   "metadata": {},
   "source": [
    "\n",
    "#### WRITE HERE THE OBSERVATIONS FOR THIS LAB\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
